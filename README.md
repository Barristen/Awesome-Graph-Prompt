# Awesome-Graph-Prompt [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)


A collection of AWESOME things about performing **Pre-training and Prompt on Graphs**.

Recently, the workflow of **"pre-training and fine-tuning"** has been proved less effective and efficient when applied to diverse graph downstream tasks.
Inspired by the prompt learning in natural language processing (NLP), the **"pre-training and prompting"** workflow has emerged as a promising solution. This repo aims to provide a curated list of research papers that explore the prompting on graphs.


## Table of Contents

- [Awesome-Graph-Prompt](#awesome-graph-prompt-awesomehttpsawesomerebadgesvghttpsawesomere)
  - [Table of Contents](#table-of-contents)
  - [Survey](#survey)
  - [Papers](#papers) 
    - [2023](#2023)
    - [2022](#2022)
  - [Contributing](#contributing)




## Survey

* A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges (*May 2023, arXiv*) [[Paper](https://arxiv.org/abs/2303.07275)]




## Papers

### 2023

* Universal Prompt Tuning for Graph Neural Networks (***July 2023, arXiv***) [[Paper](https://arxiv.org/abs/2209.15240)]
* Virtual Node Tuning for Few-shot Node Classification (***KDD'2023***) [[Paper](https://arxiv.org/abs/2306.06063)]
* All in One: Multi-Task Prompting for Graph Neural Networks (***KDD'2023***) [[Paper](https://arxiv.org/abs/2307.01504 )]  [[Code](https://github.com/sheldonresearch/ProG)]
* PRODIGY: Enabling In-context Learning Over Graphs (***May 2023, arXiv***) [[Paper](https://arxiv.org/abs/2305.12600)]
* GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks (***WWW'2023***) [[Paper](https://dl.acm.org/doi/10.1145/3543507.3583386 )] [[Code](https://github.com/Starlien95/GraphPrompt )]



### 2022

* GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks (***KDD'2022***) [[Paper](https://dl.acm.org/doi/10.1145/3534678.3539249 )]  [[Code](https://github.com/MingChen-Sun/GPPT)]


## Contributing
üëç Contributions to this repository are welcome! 

If you have come across relevant resources, feel free to open an issue or submit a pull request.
```
- paper_name (***journal***) [[Paper](link)] [[Code](link)]
```
